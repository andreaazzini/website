---
title: Materials and Methods
---

After having reviewed the concepts that constitute the basic knowledge
for the comprehension of the experiments we have performed, let’s dive
into a more practical and concrete topic, which concerns the materials
and methods we have used in order to conduct our research.

### Description

Our work was focused on creating a fully-convolutional pipeline that
could classify, segment and *quantify* objects. About what we mean with
the term *quantify* we will elaborate in the following sections.
Furthermore, we performed our experiments with a fully *synthetic*
training set, and testing the performance both on *synthetic* and *real*
images. Again, we are going to clarify this terminology in the following
sections. We collected and analyzed the results this way in order to
understand if, by any chance, there is room for generalization in a
problem such as *Deep Learning Quantification*. We have built models
that are able to generalize under certain conditions, and analyzed the
cases where the same models lacked generalization abilities. We finally
provided a novel architecture to solve both the faced problems with a
single network, using a multitask learning approach.

To summarize, our contributions can be divided in 3 main subcategories:
segmentation, quantification, novel architecture. Before describing the
experiments in detail, it is necessary we resolve every ambiguity in
terminology.

#### Terminology

The terms we use from now on should be clear and unambiguous for the
reader. Let’s provide an explanation for the most frequent and important
ones.

-   ***Quantification***: in the following sections, we are going to
    talk about something we call *quantification* a lot. What we mean by
    that is the ability to *quantify* how much of an object there is in
    a picture. Our models, ultimately, need to learn how to reconstruct
    a 3D model from 2D images of the same object, and try to predict the
    volume of these objects.

-   ***Synthetic vs Real***: there are two hyperclasses of images we
    have used in order to carry out our experiments: *synthetic* and
    *real* images. *Synthetic* images are the ones that have been
    generated by means of a 3D modeling and rendering software.
    Synthetic images can be generated using multiple textures, lights,
    cameras, animations and so many more parameters that we can prove
    that, under certain conditions and tasks, they can mock real images
    with a good degree of accuracy. Real images, on the other hand, are
    images that portrait something real, which is not generated, but
    rather captured from a real life scene. If synthetic are very useful
    to generate thousands of images and train bigger and bigger models,
    real images have the power to determine if the models can be good at
    predicting not only similar *fake* images, but also real images,
    i.e. the ones everybody can take pictures of with their smartphones.
    Trying to leverage these two aspects translates into practical
    considerations. In fact, if showing a working pipeline can already
    be interesting from a research point of view, making sure it works
    in real life scenarios is much more powerful to confirm that the
    research has an applicative face to be taken care of.

-   ***Generalization***: when we talk about generalization, we mean the
    ability to predict in the context of real images, while the models
    were trained on synthetic images. In fact, if the problem of making
    prediction on synthetic images already can be considered trivial, it
    is much more interesting to see if, from the synthetic world, it is
    possible to generalize to the real world.

-   ***Voxel representation***: this is a way to represent 3D objects,
    which is alternative to polygonal representation. In polygonal
    graphics, shapes are made by intersections of polygons. Polygons are
    made by filling the space defined by at least three points. In these
    technologies, points are not visible, they do not have size. On the
    other hand, voxel technology is based on visible points in space,
    that are added near each other to form complex shapes. The higher
    the number of points and the smaller their size, the higher the
    resolution. Voxel representation is used in many context, and
    objects represented this way are easier for search engine based on
    shape. This representation also makes easier to quantify the model
    proposed, by simply counting the total pixels.

-   ***Prior-based Vs non-prior based approaches***: prior based
    techniques assume that a prior knowledge of the object to
    reconstruct is given. Thus, the task is just to refine some areas of
    the object according to the given views, with an implicit knowledge
    about which position each view represents. This further information
    makes the task easier, and fewer images are required, because there
    is less need to find feature correspondence across views. There are
    methods to reconstruct shapes from priors and views based on
    elaborated formulas. In this thesis, we refined a technique based on
    neural networks, which instead of trying to adapt a prior 3D model
    to some views based on mathematical operations, is based on learning
    a mapping from the observations to the underlying 3D shapes through
    a data-driven approach.

-   ***Single vs multi-view reconstruction***: there are different kinds
    of techniques, based on how many views are used. Some techniques
    require a fixed number of views, while for some other ones, the more
    the views, the better the precision. In addition, there exist
    techniques that require at least a certain number of views and
    techniques that work even with a single view. The latter are usually
    prior-based. Indeed, the prior information can counterbalance the
    lack of views.

-   ***ShapeNet*** ShapeNet [[7](references#chang2015shapenet)][[53](references#wu20153d)] is a
    richly-annotated, large-scale dataset of 3D shapes. It is a
    collaborative effort between researchers at Princeton, Stanford and
    TTIC. ShapeNet is organized according to the WordNet hierarchy. Each
    meaningful concept in WordNet, possibly described by multiple words
    or word phrases, is called a *synonym set* or *synset*. There are
    more than 100,000 synsets in WordNet, the majority of them being
    nouns (80,000+).

#### Objectives

As hinted in the previous section, there are two main objectives we
identified as relevant for our experiments.

-   **Architecture**: we want our models to be completely based on deep
    learning techniques and architectures. Of course, there are
    different tasks to be carried out before achieving the ultimate goal
    of quantification, but each task needs to be performed by a modular
    component of a neural network.

-   **Classification**: we want to build models that can make a
    preliminary distinction of the objects they are dealing with. This
    is not the ultimate goal, but it will be clear how distinguishing
    the objects can be useful both for the neural network and the
    applicative aspects of our research.

-   **Quantification**: we want to build models that can analyze 2D
    views of various classes of objects, reconstruct their 3D shape and
    predict their volume with a high level of accuracy. We chose to
    reconstruct the volume and use it as the intermediate measurement to
    obtain all the other quantitative information. For instance, given
    the absolute value of an ingredient’s volume, it is easy to obtain
    its total weight, by using its basis weight that is already known.
    Then, given the weight of an ingredient, it’s a deterministic step
    to find the calories of the same ingredient, and so on for all the
    other measures.

-   **Generalization**: we want our models not only to perform well on
    the same kind of images (in our case, synthetic images), but also on
    real images, without making the model any bigger, deeper or more
    complex. We want to study if a sort of *natural* generalization
    arises, i.e. if the models that have been trained on synthetic
    images can make predictions on real images.

The problem we try to solve is relevant and interesting for multiple
reasons. First, quantifying objects can be useful in various scenarios,
for instance:

-   **Cuisine**: while preparing a recipe, you might not only want to
    know how much your ingredients weight, but also their nutritional
    values, and other more sophisticated information.

-   **Crop Growth**: there are a lot of issues in agriculture that
    relate to quantification. Having multiple views of the same crop can
    help predicting the status and pace of growth of a crop.

-   **Fashion**: given different pictures of a body type, it is possible
    to predict which size of particular clothes is the one that will fit
    that specific body type.

Furthermore, the generalization aspect of the research should not be
underestimated. By using synthetic images only, it is possible, in
certain scenarios, to draw conclusions (inference and prediction) over
real images, even when they weren’t involved in the training process.
This fact has a very important consequence: for simple problems, it may
be absolutely unreasonable to spend too much time retrieving huge amount
of data, when it is easier to generate it and possibly enrich it with
real-life data.

#### Proposed Pipeline

The pipeline we propose in order to achieve the aforementioned
objectives is relatively easy from a conceptual standpoint. It is mainly
divided into two distinct parts: an autoencoder performing pixel-wise
semantic segmentation on images and a 3D model reconstructor. Both the
main blocks are implemented using only neural network techniques, and
this makes possible a combination of the two networks, with some
implications that we discuss in Section \[sec:extensions\].

Here we describe how the proposed pipeline solves the problem of
quantification, from the input, which is fed to our models in form of
pictures of food ingredients, to the output, the absolute measure of the
volume of those ingredients. The output measure is then used for
computing any other related measures, such as weight, nutritional
values, and so on.

To devise a method to get the absolute volume of objects, we had to
solve two problems:

-   **Shape Reconstruction**: retrieving the exact 3D shape of objects.

-   **Absolute volume computation**: find the absolute value (in $cm^3$)
    of this shape.

For the first issue, we chose a particular network to reconstruct the 3D
shape of objects given some 2D views which has many advantages, that we
discuss in the following sections. However, it has been initially
devised to reconstruct the shape of single objects, and it does so by
filling the entire $n \\times n \\times n$ tridimensional grid space with the object to
reconstruct. This raises a challenge for the second issue, i.e. finding
the absolute volume.

In fact, the way we decided to compute the absolute value is by means of
a standard object, of known volume, which we use as a basis for
comparison with the other reconstructed objects. In this thesis, we
chose a Stabilo highlighter as known object. Therefore, the object to
reconstruct, e.g. a banana, has to be reconstructed in the same scene
with the Stabilo, and the relative proportions has to be maintained.
This is why we trained and used the 3D reconstructor in a novel way, and
we proved its capability to reconstruct multiple objects and keep their
proportions, after being trained properly with a dataset we synthesized
for these very purposes. At this point, all the elements for finding the
absolute value are ready.

In fact, given a reconstructed scene with a Stabilo and an object, it is
enough to use a simple proportion relating the object values in voxel
with their real values:

$$\\frac{Stabilo Voxels}{Stabilo Volume} = \\frac{Object Voxels}{ObjectVolume}$$

However, there is still an unsolved problem. We want our pipeline to be
fully automated, without needing any human intervention. However, there
is still the problem of counting the voxels assigned to the Stabilo
separately from the reconstructed object’s voxels. It is not easy to
separate the voxels automatically, even though at a human glance it is
immediately clear which voxel belongs to which model.

We solve this problem by leveraging the information in the segmentation
phase. In fact, from this phase it is not difficult to retrieve the
ratio between the height of the object to reconstruct and the Stabilo’s
height, given some basic assumptions on the object position. Moreover,
by means of segmentation it is also possible to obtain a 2D image
containing the object without the Stabilo, starting from the one
containing both. At this point, we can feed our 3D reconstructor with
both these inputs, thus obtaining two 3D reconstructions, the object
with the Stabilo, and the object alone. By subtracting the voxels of the
latest reconstruction to the former, we obtain the number of voxels
assigned to the Stabilo, that we can compare with the real volume of the
Stabilo. Then, it’s just a matter of proportion to obtain all the other
measurements. However, before performing this step, the scene with the
object alone has to be scaled by the ratio:

$$\\frac{Object Height}{ObjectHeight +Stabilo Height}$$

This scaling is necessary because, as stated above, the reconstructor
always tries to fill the $nxnxn$ grid space when reconstructing objects,
so to have the height of the single object to the same scale of the
object with the Stabilo, the scaling operation is needed. The pipeline
just described is represented in the picture below.

![The full pipeline](/assets/images/full_pipeline.png)

In the following sections, and in Chapter 4, we
explain how we set, trained and fine-tuned the main components of the
pipeline we have just described.

### The Dataset

Before analyzing how the pipeline is structured and the models are
built, let’s look at how the dataset has been generated. In this
section, we will document the exact, precise and unambiguous specifics
of our data. We will provide explanation about why we have made specific
decisions during the creation of the dataset. We will finally provide
comments about how the shape of our data could have influenced the
results of our models. We chose *food* as main subject of our data.

### Size, Shape and Classes

In order to carry out the preliminary experiments and to understand the
feasibility of our ideas, we created a full dataset with images that all
shared the same features. As a matter of facts, we require our images to
be all fed into a neural network, and as such, they are constrained to
be equal in size, shape and other attributes.

Also, since we are talking about images, they need to encode information
which is qualitatively understandable as a neural network input. Being
too small, the images would be more difficult to find meaningful
features in, but being too large, the images would be extremely hard to
deal with, when it comes to manipulating them. We then decided to choose
a size of $224 \\times 224$ pixels for each image of our dataset. This shape is
consistent with the literature
[[28](references#krizhevsky:2012)][[46](references#simonyan:2014)][[49](references#szegedy:2014)], and it has already
proved that it is a sufficient size for a convolutional neural network.
The synthetic images are then generated directly considering this
aspect, while real images are cropped to fit this requirement.

The real image crop is not random, nor trivially centered, but every
test image we have considered has been cropped by hand, in order not to
lose any information about the object we wanted to recognize, segment
and reconstruct. About the number of classes we wanted to work with, we
decided to include five classes of distinct objects: banana, egg, lemon, orange and pear.

This choice has been made by keeping in mind that nothing has to be
gained by increasing the number of classes we consider. We want to focus
on innovative problems of segmentation and 3D model reconstruction,
rather than classification, which is already very trivial when it comes
to image recognition. Moreover, the choice of the ingredients is not
random. Banana and lemon share the same color, therefore the network is
forced to use information non-color related. Lemon and orange share a
same porosity, thus making the learning task harder. Pear comes with
many shapes and colors, so it’s a challenging object by itself. Egg is
sometimes very pale, so it’s hard to distinguish it from white
backgrounds that are common.

#### Rendering Images

The dataset we wanted to create had to be fully-synthetic, at least on
the training side. In order to do this, keeping also generalization in
mind, we contacted an expert of image rendering, Marta
Confalonieri, in order to work out together all the little details we
needed to consider when generating the images. Having chosen the classes
of the objects and the shape of the images, we decided to grab 20
different textures for each of those objects, and feed them to a 3D
rendering software.

The software is then responsible to generate a random animation, with
different textures and variations in the shape of the 3D objects, saving
the images it generates at a predefined FPS metric. Additionally, the
software is able to generate the label images for the segmentation task
instantly, just by coloring the pixels in a different way, depending on
the object that is portrayed in the image. Different textures are
powerful, but certainly do not catch the full complexity of real images,
the ones we could easily make with our smartphones or cameras. We
realized we needed to put some effort on a more complicated variations
for our training set, enriching it with shadows, different lights from
different viewpoints, and other realistic alterations. The rendered
images are then analyzed separately.

Since the rendered images all share the same empty background, it is
arguable that, if the background were slightly different, it would
undermine the results of the training process. We used a script to put
random noise and images in place of the monochromatic backgrounds.
Intuitively, this operation forces the neural network to learn the
features of the objects it needs to detect, instead of other irrelevant
information.

Finally, we have set single objects and multiple objects apart into two
different categories during our preliminary experiments, because we
noticed it was extremely hard for our autoencoders to work with objects
that we defined as belonging to the same hyperclass, but that the
network clearly would devise as different. Only when the model got
better at generalizing to real images, we put different objects with
different multiplicities back together in the same training set for our
final experiments.

### Segmentation

The table below lists the different training sets we have
created for our diverse set of segmentation experiments, each one adding
a new kind of complexity, aimed at improving our model one step at a
time.

|          |    multiplicity    |    complex variations    |    background    |
| -------- | ------------------ | ------------------------ | ---------------- |
|    1     |       single       |            no            |  monochromatic   |
|    2     |       single       |            no            |      random      |
|    3     |      multiple      |            no            |  monochromatic   |
|    4     |      multiple      |            no            |      random      |
|    5     |  single/multiple   |            no            |  monochromatic   |
|    6     |  single/multiple   |           yes            |  monochromatic   |
|    7     |       mixed        |           yes            |  monochromatic   |

![Dataset](/assets/images/collage.png)

The problem of doing pixel-wise semantic segmentation with deep learning
is not new.
[[40](references#noh2015learning)][[5](references#badrinarayanan2015segnet)][[13](references#dai2016instance)][[33](references#lin2016efficient)]
Convolutional autoencoders have been used costantly and consistently in
all the papers that investigate the problem. However, each solution can
vary in a few details and design choices, as we will elaborate in the
following paragraphs.

Based on the SegNet model [[5](references#badrinarayanan2015segnet)], we have developed
two different kinds of encoder-decoder architectures (we will use the
term *autoencoder* interchangeably):

-   **SegnetAutoencoder**: an autoencoder that operates with the same
    number of layers of convolution, pooling, transpose convolution and
    unpooling. The only difference between this kind of autoencoder and
    the original SegNet autoencoder is the different unpooling method,
    which we will elaborate in the following paragraph.

-   **MiniAutoencoder**: an autoencoder that makes a compromise between
    training time and accuracy. Unfortunately, the SegnetAutoencoder is
    very heavy, and required roughly 6 hours to complete a training
    task. On the contrary, we have designed an architecture that reaches
    very similar accuracy levels, almost halfing the training time.

These models only differ in complexity, because MiniAutoencoder is built
using the same theoretical concepts behind SegNetAutoencoder. However,
to enhance the results of the generative side of the autoencoders, some
improvements have been implemented.

First, instead of the unpooling layers, that are typical of the
fully-convolutional autoencoders, we use strided convolutional layers.
In fact, autoencoders, similarly to generative models, tend to produce
patterned results, even when their accuracy reaches very high levels.
[[41](references#odena2016deconvolution)] These patterns are produced because of the
typical dimensionality problems with pooling and unpooling layers. In
order to overcome the problem, Odena et al. propose strided convolutions
instead of unpooling layers, which are not only producing patterned
results, but are also very expensive computationally. As a matter of
fact, two kinds of unpooling layers have been tested during our model
definition: argmax-based unpooling layers [[5](references#badrinarayanan2015segnet)],
and simpler *naive* unpooling layers, also known as upsampling layers,
which only scale their inputs as if they were images, through algorithms
such as nearest neighbor. Both of them achieved good results, without
being very good at generating pattern-free images.

By using strided convolutions, we have been able not only to solve this
problem, but also to incredibly speed up the training of the networks.
We are confident that this kind of encoder-decoder architecture can work
in more complicated settings, with more objects to be recognized, bigger
images, a greater number of textures, features, and so on. However,
several complications in the results of the experiments will make us
choose to slightly change our pipeline to cope with other flaws that can
potentially increase with the scale of the problem.

The code of our segmentation module is available on GitHub [[3](references#segnet2017)]
as a library for SegNet-like encoder-decoder architectures. As such, it
can be used in a numerous variety of settings, and it is currently used
and appreciated by PhD students and researchers around the world.

#### Classification

During the execution and analysis of the experiments, it was immediately
evident how the autoencoders were great at defining the contour and
shapes of each object, but not so precise when it came to classifying
it. In fact, autoencoders for segmentation work in a pixel-wise fashion,
and tend to be confused when labeling pixels that come from ambiguous
features. For instance, a portion of an image could contain a feature
that could be confused between lemon and banana, which can be similar
objects in color and texture. As it will be evident when dealing with
the result, this does not affect the overall result of the segmentation,
i.e. the object versus background problem. However, in order to quantify
an object, it is definitely useful to know which object we deal with
entirely, without any ambiguity. There are two possible ways to face and
solve the problem, one based on the installation of an image classifier
within the pipeline, the other that requires a more complex training
set, but insists on pushing forward the autoencoder capabilities.

The use of an image classifier is extremely trivial, as we have seen
that image recognition is incredibly good using neural network, even
with enormous data to be classified. However, we have conducted
experiments on the segmentation classification, which seemed more
interesting and it is not very supported by the literature.

### Volume Reconstruction

In this section, we first discuss the main network we used for the
problems related to quantification. We then discuss two sub-problems,
the problem of finding an accurate volume shape and the problem of
relating a relative shape to an absolute measurement. The combination of
both solves the Quantification problem. The results of the experiments
that we discuss here are shown in the next section.

#### 3D Recurrent Reconstruction Neural Network

We surveyed many different 3D shape reconstruction techniques, based on
neural networks
[[51](references#tatarchenko2016multi)][[9](references#chen2016multi)][[36](references#liu2016upright)][[44](references#sedaghat2016orientation)],
looking for the best one to adapt to our needs. We mainly reviewed
techniques based on depth estimation, 3D orientation and 3D shape
reconstruction. We concluded that a depth estimation map, as well as
information about the 3D orientation, were not enough to solve the
quantification problem, so we narrowed down our focus to volume
reconstruction techniques. The reason why we think the complete volume
reconstruction fits our needs are described in section
\[sec:proposed\].

As a reconstructor, we eventually chose the 3D Recurrent Reconstruction
Neural Network recently proposed at Stanford [[11](references#choy20163d)], which we now
describe in its main features. We chose this network for many reasons:

-   it uses a recurrent neural network to increasingly improve the
    quality of the reconstruction. In principle, it can work even with a
    single view, thus in our use case scenarios it gives more
    flexibility to the final users, that can decide how many pictures to
    take.

-   it does not need a label-specific training, so it is quite flexible
    even with classes of unseen objects, though it really shines when it
    is tested on the classes is trained with.

-   even though it performs better with segmented images, it is
    sometimes also robust with unsegmented ones, thus decreasing the
    need for a perfect segmentation for each test image and increasing
    the overall robustness of the pipeline.

-   it does not rely on common assumptions such Lambertian appearances
    of objects and non-uniform albedos. Thus, it is more robust to
    self-occlusions, and local appearance changes.

We provide here a general description of the network, then we dive into
the description of the problems we studied.

The network learns a mapping from 2D rendered images of objects to their
underlying 3D shapes. The network takes in one or more images of an
object instance from arbitrary viewpoints and outputs a reconstruction
of the object in the form of a 3D occupancy grid. It mainly leverages
three concepts: convolution, deconvolution and recurrency, which are
implemented by the following three components:

-   **2D Convolutional Neural Network (2D-CNN)**: given one or more
    images of an object from arbitrary viewpoints, it encodes each input
    image $x$ into low dimensional features $T(x)$.

-   **3D Convolutional LSTM (3D-LSTM)**: this component is designed to
    retain previous observations and incrementally refine the output
    reconstruction as more observations become available. It takes the
    output of the previous component and it either selectively updates
    its cell states or retains the states by closing the input gate.

-   **3D Deconvolutional Neural Network (3D-DCNN)**: it decodes the
    hidden states of the LSTM units and generates a 3D probabilistic
    voxel reconstruction.

The loss function of the network is defined as the sum of voxel-wise
cross-entropy:

$$L(\\mathcal{X}, y) = \\sum\_{i,j,k}{y\_{(i,j,k)}log(p\_{(i,j,k)}) + (1 - y\_{(i,j,k)})log(1 - p\_{(i,j,k)})}$$

#### Optimization of Volume Reconstruction

In this part of our work, our goal was to accurately reconstruct the
objects in our chosen ingredient classes. Therefore, the training set in
this section was constituted by some views of our ingredients as input,
and the model we used to generate them as the expected output. To better
understand the nature of the network and how to best train it, we used 3
kind of training sets:

-   **Generic classes from ShapeNet**: the purpose of this test is to
    see how the network can generalize to unseen kinds of objects.

-   **Our specific ingredient classes**: the straightforward purpose
    here is to see how much the accuracy can improve with respect to
    using non-specific classes.

-   **Generic ShapeNet with specific ingredient classes**: here we study
    whether a pre-training with generic classes and a fine tuning with
    the specific ones can outperform the training with only specific
    ones, as it happens in many classification and segmentation
    problems.

In order to leverage a considerable database of already rendered views
from ShapeNet, we decided to keep the same format in our novel training
set.

Then, regarding our specific objects, we created 3D models that we later
converted into voxel format, with dimension $32 \\times 32 \\times 32$. We created
approximately 20 different models for each classes. These models are the
*labels* in our supervised problem. For each model, we used five
different textures, and for each combination of the model-texture
couples we generated $50$ views, thus obtaining $100$ unique objects with $50$
views per object.

### Absolute Volume Measurement

After being confident in the accuracy of the volume reconstruction,
there is still a problem to be solved. The output of the prediction
obtained with the first database, is the volume of the object tested in
the $32 \\times 32 \\times 32$ space. But this is only a relative measurement,
especially since the object is separated from the others in our
pipeline, before being reconstructed.

However, to really being able to quantify objects, we need their
absolute values. To solve this problem, we thought about a few
approaches, such as:

-   Obtaining a depth estimation map of pictures taken from a standard
    distance. This kind of approach has two drawbacks. It require
    additional work for the depth map, and it gives users a rigid
    constraint to take pictures from the same distance.

-   Including in the pictures an object common in all kitchens and with
    a standard size, such as a spoon, and use it as a metric to compare
    it to the size of the other ingredients.

We chose the second approach because it does not require particular
efforts to the final users, and it gives them freedom to take pictures
from the position and distance they want.

Therefore, we created a dataset for two experiments. The dataset is
composed by the same object in the previouse experiments, but this time
they sometimes have in the same picture a Stabilo boss, in different
sizes.

The first experiment with this training set is about verifying whether
the network could work with two objects at the same time, and
distinguish their relative difference in size. We test if the ratio
between the Stabilo and the ingredient associated with is kept even with
novel, real world images. We can also obtain a first quantification
information by manually counting the length in voxels of the Stabilo and
the reconstructed object.

The second experiment is about the volume quantification in the way
described in the Section \[sec:proposed\], which does not require human
intervention. After the segmentation, we compute the height ratio
between the Stabilo and the object with a simple script, and we generate
two pictures, one with the Stabilo and the other without it. Then we
obtain their reconstruction, we scale the one with the object alone and
we obtain the volume by comparison.

### Extensions

In this section we propose a deep neural network based on the multitask
principle. The purpose of this network is to increase the precision of
the segmentation, the volume reconstruction and even more tasks that can
be devised in future works, by sharing some information between
different tasks.

As we have already described, both the segmentation network and the 3D
reconstruction network are based on an Encoder-Decoder architecture. The
input of each one is first compressed, in order to retain only the most
useful information, and then the decoder part uses that information to
solve the predicting task. It makes sense to suppose that tasks like
segmentation and 3D reconstruction share some similarities, especially
when performed on the same object.

Human brain often integrates information from different tasks and
domains to better perform in all of them. In the same way, we may think
that a cooperation between the learning tasks considered here may
improve the result of each one of them, by at the same time making the
overall training computationally less expensive.

This concept has been proved successful [[6](references#caruana1998multitask)] and
particularly utilized in NLP tasks [[12](references#collobert2008unified)], while it is
not so widely studied in the computer vision field.

In particular, we propose a one-to-many architecture. The proposed
network shares the same encoder, but uses two different decoders, one
for the segmentation and the other for the 3D reconstruction. The model
can also be expanded by adding additional tasks, like a pure
auto-encoding task and an object detection task. The idea behind this
architecture is that the information in the encoded part must be
semantically more accurate, it intuitively must be more precise about
the object representation, in the same way human brain has a better
understand of an object when it integrates different kinds of
information about it.

Of course, the main idea of this network is not only to share the
encoder part, but also the cost function. One way to do this is to keep
the standard cost function for each task (i.e. pixelwise error for
segmentation and 3D voxelwise softmax for volume reconstruction) and
weight them through some hyper-parameters. These hyper-parameters can be
chosen in many ways, to better optimize the overall performance of the
network. For instance, Bayesian optimization can be used to fine-tune
them, since testing each value in a certain range with grid search would
be too expensive with such enormous models.

The figure below shows a diagram of the proposed model.

![Novel architecture](/assets/images/novel.png)

Notice that further tasks may be included to increase the accuracy of
each single of them even more. Some tasks seem naturally related to each
other because of the convolutional nature of their resolution
techniques, like autoencoding and object detection.

However, even novel tasks can be added, provided that they are
associated with the nature of the quantification problem. In particular,
the freedom given by having a synthetic dataset can be leveraged in many
ways. As an example, the precise quantity associated with the volume of
the object, which is an information known in the training set, can be
leveraged for a regression task.

For completeness’ sake, we specify that multitask learning can be
approached in other ways rather than the one-to-many, for instance the
decoding part can also be shared, or there may be combinations of the
shared parts.

We narrowed the discussion on the one-to-many approach because it can be
explained in a more intuitive and human-alike terms, by considering that
the intermediate representation of the objects, at the end of the
encoding part, has to be used for many tasks, thus being more accurate
and semantically meaningful, with respect to a representation used only
for a very narrow task.

A possible issue for the architecture here just discussed is that there
may be some conflictual information in the representations that is
needed for different tasks. We leave as possible further research the
problem of accurately exploring the consequences of all the proposed
variations.
